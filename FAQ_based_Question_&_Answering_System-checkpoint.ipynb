{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\palak\\anaconda3\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\palak\\anaconda3\\lib\\site-packages (from gensim) (1.20.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\palak\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.1.2 smart-open-5.2.1\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -U gensim\n",
    "python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvTKzUjnbR_j"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1df542baab63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m## make sure you downloaded model for tokenization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#nltk.download('punkt')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import os, re, io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "## stopwords\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "## lemma functionality provide by NLTK\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "## make sure you downloaded model for lemmatization\n",
    "#nltk.download('wordnet')\n",
    "from nltk import word_tokenize\n",
    "## make sure you downloaded model for tokenization\n",
    "#nltk.download('punkt')\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "## TF_IDF for BOW\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "## cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "e2lBjjkGlWWS",
    "outputId": "485c339e-aea9-46cb-8d76-b31eb2165daa"
   },
   "outputs": [],
   "source": [
    "## mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "INPUT_DIR = r'gdrive/My Drive/colab data/COVID_FAQ_QA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q6LVF-LQbek8"
   },
   "source": [
    "## Data Collection Process\n",
    "\n",
    "It is very important process in Data Science. We should collect the data from various places and combine them for our analysis. \n",
    "\n",
    "In this notebook, I will be using COVID-19 FAQ questions from available online resources.\n",
    "  1. https://www.un.org/sites/un2.un.org/files/new_dhmosh_covid-19_faq.pdf\n",
    "  2. https://www.mohfw.gov.in/pdf/FAQ.pdf\n",
    "\n",
    "In this part, Will be using FAQ questions from Point 2 website for our entire QA similarity task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvC0iPRjtIK3"
   },
   "source": [
    "#### Download dataset from source\n",
    "\n",
    "The COVID-19 FAQ data available in online. We will be taking one of the resources available in [online]( https://www.un.org/sites/un2.un.org/files/new_dhmosh_covid-19_faq.pdf) for our FAQ question answering mechanism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ysjLn81izDnU"
   },
   "outputs": [],
   "source": [
    "## The data is taken from https://www.un.org/sites/un2.un.org/files/new_dhmosh_covid-19_faq.pdf\n",
    "## it has FAQ based question and answering for COVID-19\n",
    "\n",
    "def download_pdf_url(dataset_url, file_name):\n",
    "  response = requests.get(dataset_url)\n",
    "  pdf_content_output = None\n",
    "  with io.BytesIO(response.content) as open_pdf_file:\n",
    "      with open(file_name,'w') as obj:\n",
    "        obj.write(str(open_pdf_file))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jf8IukGbva4"
   },
   "outputs": [],
   "source": [
    "dataset_url = 'https://www.un.org/sites/un2.un.org/files/new_dhmosh_covid-19_faq.pdf'\n",
    "## download pdf from URL and save the pdf file\n",
    "download_pdf_url(dataset_url, 'new_dhmosh_covid-19_faq.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MPFiSazZiouR"
   },
   "source": [
    "#### Convert Structured output (.csv) from Unstructured document (.pdf)\n",
    "\n",
    "Our downloaded FAQ dataset is PDF file. We should always gather relevant information into structured format from the unstructured document. It is very important step in our flow. We will be following below steps,\n",
    "\n",
    "1.   Convert PDF into Text file\n",
    "2.   Formulate Question and answers from the text\n",
    "3.   Store as CSV file\n",
    "\n",
    "In this article, I used [pdftools - R module](https://github.com/ropensci/pdftools) for converting PDF to TXT file which is not covered in the below code. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iXmMSDeajHOH"
   },
   "outputs": [],
   "source": [
    "## QA will be stored as .csv file\n",
    "def extract_QA_from_text_file(INPUT_DIR, text_file_name):\n",
    "  output_file_name = 'covid_19faq.csv'\n",
    "  with open(os.path.join(INPUT_DIR, text_file_name), 'r', encoding='latin') as obj:\n",
    "      text = obj.read()\n",
    "\n",
    "  text = text.strip()\n",
    "  ## extract the question by following pattern\n",
    "  pattern = '\\n+\\s*\\d+[.](.*?)\\?'\n",
    "  question_pattern = re.compile(pattern,re.MULTILINE|re.IGNORECASE|re.DOTALL)  \n",
    "  matched_QA_positions = [(m.start(0),m.end(0)) for m in question_pattern.finditer(text)]\n",
    "  print(f\"Available no of question is {len(matched_QA_positions)}\")\n",
    "  ## store question and answer pair\n",
    "  questions = {}\n",
    "  ## iterate every matched QA \n",
    "  for index in range(len(matched_QA_positions)):\n",
    "      ## get the start and end position\n",
    "      faq_start_pos = matched_QA_positions[index][0]\n",
    "      faq_end_pos = matched_QA_positions[index][1]\n",
    "      \n",
    "      if index == len(matched_QA_positions) - 1:\n",
    "          next_faq_start_pos = -1\n",
    "      else:\n",
    "          next_faq_start_pos = matched_QA_positions[index+1][0]\n",
    "\n",
    "      ## get the question from start and end position from original text      \n",
    "      question = text[faq_start_pos:faq_end_pos]\n",
    "      if next_faq_start_pos == -1:\n",
    "          answer = text[faq_end_pos:]\n",
    "      else:\n",
    "          answer = text[faq_end_pos:next_faq_start_pos]\n",
    "      ## replace multiple new lines to space in questions and answers\n",
    "      question = re.sub(\"\\n+\",\" \",question.strip())\n",
    "      answer = re.sub(\"\\n+\",\" \",answer.strip())\n",
    "      questions[question] = answer\n",
    "      \n",
    "  ## create dataframe from key-value pair\n",
    "  faq_df = pd.DataFrame.from_dict(questions, orient='index', columns=[\"answers\"])\n",
    "  faq_df[\"questions\"] = faq_df.index\n",
    "  faq_df.reset_index(inplace=True)  \n",
    "  faq_df[[\"questions\", \"answers\"]].to_csv(os.path.join(INPUT_DIR, output_file_name),index = False)\n",
    "  print(f\"COVID QA file {output_file_name} created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "sXS5wz-SvKxD",
    "outputId": "d69f55fc-1a69-4933-89ad-27a2e5dfe813"
   },
   "outputs": [],
   "source": [
    "## Converted PDF to .txt file using pdftools in R and stored in my google drive\n",
    "## create a question-answer pair in csv\n",
    "extract_QA_from_text_file(INPUT_DIR, 'new_dhmosh_covid-19_faq.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EDDtqKvJYPXY"
   },
   "source": [
    "We've created CSV file, which contains 39 different FAQ questions about COVID-19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "odS8154NnMot"
   },
   "source": [
    "#### Take a look at COVID QA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "1rOCZ9djnRwL",
    "outputId": "d185d0c5-a84c-479f-e8b7-5a987ddfc9f9"
   },
   "outputs": [],
   "source": [
    "QA_df = pd.read_csv(os.path.join(INPUT_DIR, \"covid_19faq.csv\"))\n",
    "print(\"Dimension of dataset \",QA_df.shape)\n",
    "print(\"Available columns \",QA_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "qKbrh3iwnaC7",
    "outputId": "77075968-2a30-4c78-9094-8f9a9e251ff1"
   },
   "outputs": [],
   "source": [
    "## Take a look at first 10rows\n",
    "QA_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TqywB4Z4nDE5"
   },
   "source": [
    "## Preprocessing Techniques\n",
    "\n",
    "Next step, We will not use the data as it is. Preprocessing is another very important step to fine-tune the dataset.\n",
    "\n",
    "1.   Remove unwanted characters\n",
    "2.   Remove Question number\n",
    "3.   Remove stopwords\n",
    "4.   Lemmatization - to reduce inflection of words and minimize the word ambiguity. \n",
    "\n",
    "**Why I chosen lemmatization over stemming?**\n",
    "Lemmatization is powerful operation as it takes into consideration of morphological analysis of the word. Example: bicycles or bicycles are converted to bicyles.\n",
    "But, stemming algorithm works by predefined rules to remove prefix or suffix of the word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "baCEho9EMDnK"
   },
   "outputs": [],
   "source": [
    "## Data Preprocessing\n",
    "class TextPreprocessor():\n",
    "  def __init__(self, data_df, column_name=None):\n",
    "    self.data_df = data_df  \n",
    "    if not column_name and type(colum_name) == str:\n",
    "      raise Exception(\"column name is mandatory. Make sure type is string format\")\n",
    "    self.column = column_name\n",
    "    self.convert_lowercase()    \n",
    "    self.applied_stopword = False\n",
    "    self.processed_column_name = f\"processed_{self.column}\"\n",
    "  def convert_lowercase(self):\n",
    "    ## fill empty values into empty\n",
    "    self.data_df.fillna('',inplace=True)\n",
    "    ## reduce all the columns to lowercase\n",
    "    self.data_df = self.data_df.apply(lambda column: column.astype(str).str.lower(), axis=0)    \n",
    "\n",
    "  def remove_question_no(self):\n",
    "    ## remove question no        \n",
    "    self.data_df[self.column] = self.data_df[self.column].apply(lambda row: re.sub(r'^\\d+[.]',' ', row))    \n",
    "      \n",
    "  def remove_symbols(self):\n",
    "    ## remove unwanted character          \n",
    "    self.data_df[self.column] = self.data_df[self.column].apply(lambda row: re.sub(r'[^A-Za-z0-9\\s]', ' ', row))    \n",
    "\n",
    "  def remove_stopwords(self):\n",
    "    ## remove stopwords and create a new column \n",
    "    for idx, question in enumerate(self.data_df[self.column]):      \n",
    "      self.data_df.loc[idx, self.processed_column_name] = remove_stopwords(question)        \n",
    "\n",
    "  def apply_lemmatization(self, perform_stopword):\n",
    "    ## get the root words to reduce inflection of words \n",
    "    lemmatizer = WordNetLemmatizer()    \n",
    "    ## get the column name to perform lemma operation whether stopwords removed text or not\n",
    "    if perform_stopword:\n",
    "      column_name = self.processed_column_name\n",
    "    else:\n",
    "      column_name = self.column\n",
    "    ## iterate every question, perform tokenize and lemma\n",
    "    for idx, question in enumerate(self.data_df[column_name]):\n",
    "      \n",
    "      lemmatized_sentence = []\n",
    "      ## use spacy for lemmatization\n",
    "      doc = nlp(question.strip())\n",
    "      for word in doc:       \n",
    "        lemmatized_sentence.append(word.lemma_)      \n",
    "      ## update to the same column\n",
    "      self.data_df.loc[idx, self.processed_column_name] = \" \".join(lemmatized_sentence)\n",
    "\n",
    "  def process(self, perform_stopword = True):\n",
    "    self.remove_question_no()\n",
    "    self.remove_symbols()\n",
    "    if perform_stopword:\n",
    "      self.remove_stopwords()\n",
    "    self.apply_lemmatization(perform_stopword)    \n",
    "    return self.data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "colab_type": "code",
    "id": "c2B3i6TUnBuD",
    "outputId": "c3de53f3-4df0-4f27-f631-887b2f8d21e1"
   },
   "outputs": [],
   "source": [
    "## pre-process training question data\n",
    "text_preprocessor = TextPreprocessor(QA_df.copy(), column_name=\"questions\")\n",
    "processed_QA_df = text_preprocessor.process(perform_stopword=True)\n",
    "print(processed_QA_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCV3DVFKP1xE"
   },
   "source": [
    "## TF_IDF Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j1ePFrc11bXf"
   },
   "outputs": [],
   "source": [
    "class TF_IDF():\n",
    "  def __init__(self):\n",
    "    self.dictionary = None    \n",
    "    self.model = None\n",
    "    self.bow_corpus = None\n",
    "\n",
    "  def create_tf_idf_model(self, data_df, column_name):\n",
    "    ## create sentence token list\n",
    "    sentence_token_list = [sentence.split(\" \") for sentence in data_df[column_name]]\n",
    "    \n",
    "    ## dataset vocabulary\n",
    "    self.dictionary = Dictionary(sentence_token_list) \n",
    "\n",
    "    ## bow representation of dataset\n",
    "    self.bow_corpus = [self.dictionary.doc2bow(sentence_tokens) for sentence_tokens in sentence_token_list]\n",
    "\n",
    "    ## compute TF-IDF score for corpus\n",
    "    self.model = TfidfModel(self.bow_corpus)\n",
    "\n",
    "    ## representation of question and respective TF-IDF value\n",
    "    print(f\"First 10 question representation of TF-IDF vector\")\n",
    "    for index, sentence in enumerate(data_df[column_name]):\n",
    "      if index <= 10:\n",
    "        print(f\"{sentence} {self.model[self.bow_corpus[index]]}\")\n",
    "      else:\n",
    "        break\n",
    "\n",
    "  def get_vector_for_test_set(self, test_df, column_name):\n",
    "    ## store tf-idf vector\n",
    "    testset_tf_idf_vector = []\n",
    "    sentence_token_list = [sentence.split(\" \") for sentence in test_df[column_name]]\n",
    "    test_bow_corpus = [self.dictionary.doc2bow(sentence_tokens) for sentence_tokens in sentence_token_list]   \n",
    "    for test_sentence in test_bow_corpus:\n",
    "      testset_tf_idf_vector.append(self.model[test_sentence])      \n",
    "    \n",
    "    return testset_tf_idf_vector\n",
    "\n",
    "  def get_training_QA_vectors(self):\n",
    "    QA_vectors = []\n",
    "    for sentence_vector in self.bow_corpus:\n",
    "      QA_vectors.append(self.model[sentence_vector])      \n",
    "    return QA_vectors\n",
    "  \n",
    "  def get_train_vocabulary(self):\n",
    "    vocab = []\n",
    "    for index in self.dictionary:\n",
    "      vocab.append(self.dictionary[index])\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qj6fTgwJ7pPu"
   },
   "outputs": [],
   "source": [
    "## helps to retrieve similar question based of input vectors/embeddings for test query\n",
    "def retrieveSimilarFAQ(train_question_vectors, test_question_vectors, train_QA_df, train_column_name, test_QA_df, test_column_name):\n",
    "  similar_question_index = []\n",
    "  for test_index, test_vector in enumerate(test_question_vectors):\n",
    "    sim, sim_Q_index = -1, -1\n",
    "    for train_index, train_vector in enumerate(train_question_vectors):\n",
    "      sim_score = cosine_similarity(train_vector, test_vector)[0][0]\n",
    "     \n",
    "      if sim < sim_score:\n",
    "        sim = sim_score\n",
    "        sim_Q_index = train_index\n",
    "    \n",
    "    print(\"######\")\n",
    "    print(f\"Query Question: {test_QA_df[test_column_name].iloc[test_index]}\")    \n",
    "    print(f\"Retrieved Question: {train_QA_df[train_column_name].iloc[sim_Q_index]}\")\n",
    "    print(\"######\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "id": "b771vSXo3sm-",
    "outputId": "e6583a76-6e67-4b03-bee6-46b0ae2df576"
   },
   "outputs": [],
   "source": [
    "tf_idf = TF_IDF()\n",
    "tf_idf.create_tf_idf_model(processed_QA_df, \"processed_questions\")\n",
    "## get the tf-idf reprentation \n",
    "question_QA_vectors = tf_idf.get_training_QA_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mil0SPILkJ86"
   },
   "outputs": [],
   "source": [
    "document_vocabulary = tf_idf.get_train_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ExIMPp5rgERb"
   },
   "source": [
    "#### Evaulaute TF-IDF representation for QA similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "XDD4VQ_ahr-R",
    "outputId": "1a477cc5-b49f-4d83-9ea7-3930165eabf1"
   },
   "outputs": [],
   "source": [
    "test_query_string = [\"how does covid-19 spread?\", \n",
    "                     \"What are the symptoms of COVID-19?\",\n",
    "                \"Should I wear a mask to protect myself from covid-19\",              \n",
    "                \"Is there a vaccine for COVID-19\",\n",
    "                \"can the virus transmit through air?\",\n",
    "                \"can the virus spread through air?\"]\n",
    "\n",
    "test_QA_df = pd.DataFrame(test_query_string, columns=[\"test_questions\"])              \n",
    "## pre-process testing QA data\n",
    "text_preprocessor = TextPreprocessor(test_QA_df, column_name=\"test_questions\")\n",
    "query_QA_df = text_preprocessor.process(perform_stopword=True)\n",
    "## TF-IDF vector represetation\n",
    "query_QA_vectors = tf_idf.get_vector_for_test_set(query_QA_df, \"processed_test_questions\")\n",
    "print(query_QA_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "colab_type": "code",
    "id": "MFO0KBENMfCK",
    "outputId": "b79a8580-1f9f-4dc9-c796-c6c340f6fda7"
   },
   "outputs": [],
   "source": [
    "retrieveSimilarFAQ(question_QA_vectors, query_QA_vectors, processed_QA_df, \"questions\", query_QA_df, \"test_questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6sk8hUnelRL"
   },
   "source": [
    "## Embedding - Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-FGtL32eoh3"
   },
   "outputs": [],
   "source": [
    "## Now, Let's try building embedding based\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ehlG0CI9Y6L"
   },
   "outputs": [],
   "source": [
    "class Embeddings():\n",
    "  def __init__(self, model_path):\n",
    "    self.model_path = model_path\n",
    "    self.model = None\n",
    "    self.__load_model__()\n",
    "    \n",
    "  def __load_model__(self):\n",
    "    #word_vectors = api.load(\"glove-wiki-gigaword-100\")  \n",
    "    model_name = 'glove-twitter-25' #'word2vec-google-news-50' #'glove-twitter-25'  \n",
    "    if not os.path.exists(self.model_path+ model_name):\n",
    "      print(\"Downloading model\")\n",
    "      self.model = api.load(model_name)\n",
    "      self.model.save(self.model_path+ model_name)\n",
    "    else:\n",
    "      print(\"Loading model from Drive\")\n",
    "      self.model = KeyedVectors.load(self.model_path+ model_name)\n",
    "    \n",
    "  def get_oov_from_model(self, document_vocabulary):\n",
    "    ## the below words are not available in our pre-trained model model_name\n",
    "    print(\"The below words are not found in our pre-trained model\")\n",
    "    words = []\n",
    "    for word in set(document_vocabulary):  \n",
    "      if word not in self.model:\n",
    "        words.append(word)\n",
    "    print(words)  \n",
    "\n",
    "  def get_sentence_embeddings(self, data_df, column_name):\n",
    "    sentence_embeddings_list = []\n",
    "    for sentence in data_df[column_name]:      \n",
    "      sentence_embeddings = np.repeat(0, self.model.vector_size)\n",
    "      try:\n",
    "        tokens = sentence.split(\" \")\n",
    "        ## get the word embedding\n",
    "        for word in tokens:\n",
    "          if word in self.model:\n",
    "            word_embedding = self.model[word]\n",
    "          else:\n",
    "            word_embedding = np.repeat(0, self.model.vector_size)          \n",
    "          sentence_embeddings = sentence_embeddings + word_embedding\n",
    "          ## take the average for sentence embeddings\n",
    "          #sentence_embeddings = sentence_embeddings / len(tokens)\n",
    "        sentence_embeddings_list.append(sentence_embeddings.reshape(1, -1))\n",
    "      except Exception as e:\n",
    "          print(e)\n",
    "        \n",
    "    return sentence_embeddings_list      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "RztBewQgOsYG",
    "outputId": "e1c74e57-6f06-47ea-cd31-2c38350912cc"
   },
   "outputs": [],
   "source": [
    "## create Embedding object\n",
    "embedding = Embeddings(INPUT_DIR)\n",
    "## look for out of vocabulary COVID QA dataset - pretrained model\n",
    "embedding.get_oov_from_model(document_vocabulary)\n",
    "## get the sentence embedding for COVID QA dataset\n",
    "question_QA_embeddings = embedding.get_sentence_embeddings(processed_QA_df, \"processed_questions\")\n",
    "\n",
    "## get the sentence embedding for COVID QA query\n",
    "query_QA_embeddings = embedding.get_sentence_embeddings(query_QA_df, \"processed_test_questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AHJ7fCIdW6yr"
   },
   "outputs": [],
   "source": [
    "## Only few words are not available in pre-trained embeddings. It's fine to omit keyword like covid and 19\n",
    "## Since, I don't have much data to build own Word2Vec embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IeNJyX8gDjkp"
   },
   "source": [
    "#### Evaluate Embedding Representation for QA similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "colab_type": "code",
    "id": "g03z5tBdQeuf",
    "outputId": "4b7e4de8-d446-4fc8-a0e7-5a7e8f2e624b"
   },
   "outputs": [],
   "source": [
    "retrieveSimilarFAQ(question_QA_embeddings, query_QA_embeddings, processed_QA_df, \"questions\", query_QA_df, \"test_questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SzkULk4EEScZ"
   },
   "source": [
    "## BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aL23VcGpETJ8"
   },
   "outputs": [],
   "source": [
    "#!pip install bert-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r6Yz55DVN4In"
   },
   "outputs": [],
   "source": [
    "from bert_embedding import BertEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7cjSV2cNOG8t"
   },
   "outputs": [],
   "source": [
    "bert_embedding = BertEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VSu64mgPOMQj"
   },
   "outputs": [],
   "source": [
    "QA_questions = processed_QA_df[\"questions\"].to_list()\n",
    "query_QA_questions = test_QA_df[\"test_questions\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkW8srvGPVh3"
   },
   "outputs": [],
   "source": [
    "question_QA_bert_embeddings_list = bert_embedding(QA_questions)\n",
    "query_QA_bert_embeddings_list = bert_embedding(query_QA_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aY1n3EC5PfCr"
   },
   "outputs": [],
   "source": [
    "## store QA bert embeddings in list\n",
    "question_QA_bert_embeddings = []\n",
    "for embeddings in question_QA_bert_embeddings_list:\n",
    "  question_QA_bert_embeddings.append(embeddings[1])\n",
    "\n",
    "## store query string bert embeddings in list\n",
    "query_QA_bert_embeddings = []\n",
    "for embeddings in query_QA_bert_embeddings_list:\n",
    "  query_QA_bert_embeddings.append(embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JQDgflUZR5ln"
   },
   "source": [
    "#### Evaluate BERT embeddings for QA similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "colab_type": "code",
    "id": "j-qlpfa2P0cX",
    "outputId": "2bcc824b-66ea-4972-fd89-d48644abb5c7"
   },
   "outputs": [],
   "source": [
    "retrieveSimilarFAQ(question_QA_bert_embeddings, query_QA_bert_embeddings, processed_QA_df, \"questions\", query_QA_df, \"test_questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VGyIcEW5Qz3J"
   },
   "source": [
    "## Visualizing Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8y9nKIptO6cz"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3XV2cvGPSni6"
   },
   "outputs": [],
   "source": [
    "def display_closestwords_tsnescatterplot(model, word):\n",
    "    \n",
    "    arr = np.empty((0,25), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    # get close words\n",
    "    close_words = model.similar_by_word(word)\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "fTC7uWL7OzO6",
    "outputId": "8968da54-99be-4181-eea7-4b5474bba1e0"
   },
   "outputs": [],
   "source": [
    "display_closestwords_tsnescatterplot(embedding.model, 'prevent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "id": "EKp-fYAhPTU4",
    "outputId": "01da1eba-62af-4fa2-9bb7-74f7cd495a56"
   },
   "outputs": [],
   "source": [
    "embedding.model.similar_by_word('prevent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DlaxvlggPsCf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FAQ based Question & Answering System.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
